{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Lab - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "![Imgur](https://i.imgur.com/UB4Kg0w.jpg)\n",
    "[Link to sketchboard](https://sketchboard.me/tBjCwrhXsFwu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLP Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 1**: Clean data\n",
    "    - Remove all irrelevant characters such as any non alphanumeric characters\n",
    "    - Tokenize your text by separating it into individual words \n",
    "    - Remove words that are not relevant, such as “@” twitter mentions or urls \n",
    "    - Convert all characters to **lowercase**, in order to treat words such as “hello”, “Hello”, and “HELLO” the same \n",
    "    - Consider **lemmatization** (reduce words such as “am”, “are”, and “is” to a common form such as “be”)\n",
    "- **Step 2**: Representation\n",
    "    - Bag of Words or TFIDF\n",
    "- **Step 3**: Classification\n",
    "    - Naive Bayes\n",
    "    - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String manipulation in Python\n",
    "**Before going to NLP we need tools to handle string**\n",
    "\n",
    "One place where the Python language really shines is in the manipulation of strings. This section will cover some of Python's built-in string methods and formatting operations, before moving on to a quick guide to the extremely useful subject of regular expressions. Such string manipulation patterns come up often in the context of data science work\n",
    "\n",
    "### Formatting strings: Adjusting case\n",
    "\n",
    "Python makes it quite easy to adjust the case of a string. Here we'll look at the `upper()`, `lower()`, `capitalize()`, and `swapcase()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox.\n"
     ]
    }
   ],
   "source": [
    "fox = 'tHe qUICk bROWn fOx.'\n",
    "# Apply the functions above to `fox` and print out the results\n",
    "# Your code here\n",
    "\n",
    "lower = fox.lower()\n",
    "# lower = fox.upper()\n",
    "# lower = fox.capitalize()\n",
    "# lower = fox.swapcase()\n",
    "\n",
    "print(lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and removing spaces\n",
    "\n",
    "Another common need is to remove spaces (or other characters) from the beginning or end of the string. The basic method of removing characters is the `strip()` method, which strips whitespace from the beginning and end of the line. To remove just space to the right or left, use `rstrip()` or `lstrip()` respectively.\n",
    "\n",
    "To remove characters other than spaces, you can pass the desired character to the `strip()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the content         \n",
      "435\n"
     ]
    }
   ],
   "source": [
    "line = '         this is the content         '\n",
    "\n",
    "# Apply strip(), rstrip(), lstrip() to 'line' and print the results out\n",
    "# Your code here\n",
    "#line = line.strip()\n",
    "#line = line.rstrip()\n",
    "line = line.lstrip()\n",
    "print (line)\n",
    "num = '00000000435'\n",
    "print(int(num))\n",
    "# Remove all of the zeros from num\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding and replacing and splitting\n",
    "\n",
    "If you want to find occurrences of a certain character in a string, the `find()`, `index()` and `replace()` methods are the best built-in methods.\n",
    "\n",
    "`find()` and `index()` are very similar, in that they search for the first occurrence of a character or substring within a string, and return the index of the substring.\n",
    "\n",
    "The `split()` method is perhaps more useful; it finds all instances of the split-point and returns the substrings in between. The default is to split on any whitespace, returning a list of the individual words in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "the quick red fox jumped over a lazy dog\n",
      "['the', 'quick', 'red', 'fox', 'jumped', 'over', 'a', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "line = 'the quick brown fox jumped over a lazy dog'\n",
    "\n",
    "# Find the index of 'fox' in 'line' using find() and index()\n",
    "# Your code here\n",
    "index1 = line.find(\"fox\")\n",
    "index2 = line.index(\"fox\")\n",
    "print(index2)\n",
    "# Let's replace 'brown' with 'red'\n",
    "# Your code here\n",
    "line = line.replace(\"brown\",\"red\")\n",
    "print(line)\n",
    "# List all words in 'line' and put them in an array\n",
    "# Your 1 line of code here\n",
    "array_line = line.split(\" \")\n",
    "print(array_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you would like to undo a `split()`, you can use the `join()` method, which returns a string built from a splitpoint and an iterable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1--2--3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'--'.join(['1', '2', '3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common pattern is to use the special character \"\\n\" (newline) to join together lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rules in family:\n",
      "1. Your wife is always right.\n",
      "2. If she is wrong, check the first rule again.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(['Rules in family:', '1. Your wife is always right.', '2. If she is wrong, check the first rule again.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression\n",
    "\n",
    "In Python, regular expressions are supported by the `re` module.\n",
    "\n",
    "A regular expression (or RE) specifies a set of strings that matches it; the functions in this module let you check if a particular string matches a given regular expression (or if a given regular expression matches a particular string, which comes down to the same thing).\n",
    "\n",
    "Let's walk through some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['minhdh@coderschool.vn', 'haiminh101@yahoo.vn']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_regex = '\\w+@\\w+\\.[a-z]{2}'\n",
    "text = \"To email Hai Minh, try minhdh@coderschool.vn or the older address haiminh101@yahoo.vn\"\n",
    "re.findall(email_regex, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To email Hai Minh, try --@--.-- or the older address --@--.--'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing these email addresses with another string, perhaps to hide addresses in the output:\n",
    "re.sub(email_regex, '--@--.--', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c', 'ns', 'q', '', 'nt', '', 'l']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following will match any lower-case vowel:\n",
    "re.split('[aeiou]', 'consequential')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to extract from a document specific numerical codes that consist of a capital letter followed by a digit. You could do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G2', 'H6']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z][0-9]', '1043879, G2, H6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table lists a few of these characters that are commonly useful:\n",
    "\n",
    "| Character | Description | Character | Description |\n",
    "|------------|-----------|------------|-----------|\n",
    "| \"\\d\" | Match any digit   | \"\\D\" | Match any non-digit|\n",
    "| \"\\s\" | Match any whitespace   | \"\\S\" | Match any non-whitespace|\n",
    "| \"\\w\" | Match any alphanumeric char  | \"\\W\" | Match any non-alphanumeric char|\n",
    "\n",
    "| Character | Description | Example |\n",
    "|------------|-----------|------------|\n",
    "| ? | Match zero or one repetitions of preceding |  \"ab?\" matches \"a\" or \"ab\" |\n",
    "| * | Match zero or more repetitions of preceding | \"ab*\" matches \"a\", \"ab\", \"abb\", \"abbb\"... |\n",
    "| + | Match one or more repetitions of preceding |  \"ab+\" matches \"ab\", \"abb\", \"abbb\"... but not \"a\" |\n",
    "| {n} | Match n repetitions of preceding | \"ab{2}\" matches \"abb\" |\n",
    "| {m,n} | Match between m and n repetitions of preceding |  \"ab{2,3}\" matches \"abb\" or \"abbb\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Resources on Regular Expressions\n",
    "\n",
    "* [Python's re package Documentation](https://docs.python.org/3/library/re.html)\n",
    "* [Python's official regular expression HOWTO](https://docs.python.org/3/howto/regex.html)\n",
    "* [Mastering Regular Expressions (OReilly, 2006)](http://shop.oreilly.com/product/9780596528126.do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sentiment analysis\n",
    "This contest is taken from the real task of Text Processing.\n",
    "\n",
    "The task is to build a model that will determine the tone (positive, negative) of the text. To do this, you will need to train the model on the existing data (train.csv). The resulting model will have to determine the class (neutral, positive, negative) of new texts. The dataset contains the following fields:\n",
    "\n",
    "| Field name | Meaning |\n",
    "|------------|-----------|\n",
    "| ItemID  | id of twit|\n",
    "| Sentiment | sentiment (1-positive, 0-negative)|\n",
    "| SentimentText | text of the twit|\n",
    "\n",
    "Let's first of all have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34145</th>\n",
       "      <td>34157</td>\n",
       "      <td>0</td>\n",
       "      <td>@ajmclean_team hey we answered first !!! don`t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49238</th>\n",
       "      <td>49250</td>\n",
       "      <td>1</td>\n",
       "      <td>@amanda525 Lol, would be nice.  The pool is cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71626</th>\n",
       "      <td>71638</td>\n",
       "      <td>1</td>\n",
       "      <td>@BrianMcnugget OMG Poor sod lol. Sent the text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85907</th>\n",
       "      <td>85919</td>\n",
       "      <td>0</td>\n",
       "      <td>@audreyrey I want to but I can't afford it  st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>647</td>\n",
       "      <td>1</td>\n",
       "      <td>xD go day!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4444</th>\n",
       "      <td>4446</td>\n",
       "      <td>0</td>\n",
       "      <td>poor rob  Glad he's fine, but i'm sure it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54109</th>\n",
       "      <td>54121</td>\n",
       "      <td>0</td>\n",
       "      <td>@aussiemcflyfan its so unfortunate  it'd be go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62138</th>\n",
       "      <td>62150</td>\n",
       "      <td>1</td>\n",
       "      <td>@bitofwhimsy i walk very slowly through the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5815</th>\n",
       "      <td>5818</td>\n",
       "      <td>1</td>\n",
       "      <td>#Beijing Good massage for you &amp;amp; Sexy girl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86315</th>\n",
       "      <td>86327</td>\n",
       "      <td>1</td>\n",
       "      <td>@chadparsons Yeah i am in love with the show</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ItemID  Sentiment                                      SentimentText\n",
       "34145   34157          0  @ajmclean_team hey we answered first !!! don`t...\n",
       "49238   49250          1  @amanda525 Lol, would be nice.  The pool is cl...\n",
       "71626   71638          1  @BrianMcnugget OMG Poor sod lol. Sent the text...\n",
       "85907   85919          0  @audreyrey I want to but I can't afford it  st...\n",
       "646       647          1                                       xD go day!!!\n",
       "4444     4446          0   poor rob  Glad he's fine, but i'm sure it was...\n",
       "54109   54121          0  @aussiemcflyfan its so unfortunate  it'd be go...\n",
       "62138   62150          1  @bitofwhimsy i walk very slowly through the co...\n",
       "5815     5818          1  #Beijing Good massage for you &amp; Sexy girl ...\n",
       "86315   86327          1      @chadparsons Yeah i am in love with the show "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas, numpy and the dataset, save it in a object called 'sentiment'\n",
    "# Your code here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Read file with param './data/train.csv', encoding='latin-1'\n",
    "sentiment = pd.read_csv('./data/train.csv',encoding='latin-1') # Your code here\n",
    "\n",
    "# Let's check sentiment.head(10) and sample(10)\n",
    "# Your code here\n",
    "#sentiment.head(10)\n",
    "sentiment.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the structure of a twit varies a lot between twit and twit. They have different lengths, letters, numbers, extrange characters, etc. \n",
    "\n",
    "It is also important to note that **a lot** of words are not correctly spelled, for example the word _\"Juuuuuuuuuuuuuuuuussssst\"_ or the word _\"sooo\"_\n",
    "\n",
    "This makes it hard to mesure how positive or negative are the words within the twits.\n",
    "\n",
    "So we need a way of scoring the words such that words that appear in positive twits have greater score that those that appear in negative twits.\n",
    "\n",
    "But first... how do we represent the twits as vectors we can input to our algorithm?\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "One thing we could do to represent the twits as equal-sized vectors of numbers is the following:\n",
    "\n",
    "* Create a list (vocabulary) with all the unique words in the whole corpus of twits. \n",
    "* We construct a feature vector from each twit that contains the counts of how often each word occurs in the particular twit\n",
    "\n",
    "_Note that since the unique words in each twit represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectors will mostly consist of zeros_\n",
    "\n",
    "Lets construct the bag of words. We will work with a smaller example for illustrative purposes, and at the end we will work with our real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "twits = [\n",
    "    'This is amazing!',\n",
    "    'ML is the best, yes it is',\n",
    "    'I am not sure about how this is going to end...'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import [CountVectorizer.](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) It'll help us to convert a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define an object of CountVectorizer() as count\n",
    "count = CountVectorizer() # Your code here\n",
    "\n",
    "# With count object, fit and transfom your twits and save result in a variable name 'bag'\n",
    "# Your code here\n",
    "bag = count.fit_transform(twits) # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'am',\n",
       " 'amazing',\n",
       " 'best',\n",
       " 'end',\n",
       " 'going',\n",
       " 'how',\n",
       " 'is',\n",
       " 'it',\n",
       " 'ml',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to',\n",
       " 'yes']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find in document of CountVectorizer a function that show us list of feature names\n",
    "# hint: get_feature_names\n",
    "# Your code here\n",
    "count.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from executing the preceding command, the vocabulary is stored in a Python array that maps the unique words to integer indices. Next, let's print the feature vectors that we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call toarray() on your 'bag' to see the feature vectors\n",
    "# Your code here\n",
    "vectors = bag.toarray()\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the index of the word 'is' and how many times it occurs in all three twits?\n",
    "# Hint: You can directly count on feature fectors\n",
    "# Your answer here\n",
    "index_is = count.get_feature_names().index(\"is\")\n",
    "count_is = 0\n",
    "for vector in vectors:\n",
    "    count_is = count_is + vector[7]\n",
    "count_is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each index position in the feature vectors corresponds to the integer values that are stored as dictionary items in the CountVectorizer vocabulary. For example, the first feature at index position 0 resembles the count of the word 'about' , which only occurs in the last document. These values in the feature vectors are also called the **raw term frequencies**: `tf(t,d )` —the number of times a term `t` occurs in a document `d`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How relevant are words? Term frequency-inverse document frequency\n",
    "\n",
    "We could use these raw term frequencies to score the words in our algorithm. There is a problem though: If a word is very frequent in _all_ documents, then it probably doesn't carry a lot of information. In order to tacke this problem we can use **term frequency-inverse document frequency**, which will reduce the score the more frequent the word is accross all twits. It is calculated like this:\n",
    "\n",
    "\\begin{equation*}\n",
    "tf-idf(t,d) = tf(t,d) ~ idf(t,d)\n",
    "\\end{equation*}\n",
    "\n",
    "_tf(t,d)_ is the raw term frequency descrived above. _idf(t,d)_ is the inverse document frequency, than can be calculated as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log \\frac{n_d}{1+df\\left(d,t\\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "where `n` is the total number of documents and _df(t,d)_ is the number of documents where the term `t` appears. \n",
    "\n",
    "The `1` addition in the denominator is just to avoid zero term for terms that appear in all documents, will not be entirely ignored. Ans the `log` ensures that low frequency term don't get too much weight.\n",
    "\n",
    "Fortunately for us `scikit-learn` does all those calculations for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Formatting the number to 2 digits after the decimal point by showing on this notebook\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Feed the tf-idf Vectorizer with twits using fit_transform()\n",
    "tfidf_vec = tfidf.fit_transform(twits) # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.72, 0.  , 0.  , 0.  , 0.  , 0.43, 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.55, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.4 , 0.  , 0.  , 0.  , 0.47, 0.4 , 0.4 , 0.  ,\n",
       "        0.  , 0.4 , 0.  , 0.  , 0.4 ],\n",
       "       [0.33, 0.33, 0.  , 0.  , 0.33, 0.33, 0.33, 0.2 , 0.  , 0.  , 0.33,\n",
       "        0.33, 0.  , 0.25, 0.33, 0.  ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now what is the weight of the word 'is' and 'amazing'?\n",
    "# hint: using \n",
    "# tfidf.get_feature_names()\n",
    "# tfidf_vec.toarray()\n",
    "# Your answer here\n",
    "\n",
    "tfidf.get_feature_names()\n",
    "tfidf_vec.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data clean up\n",
    "\n",
    "### Removing stop words\n",
    "\n",
    "Now that we know how to format and score our input. Let's look at our **real** vocabulary. Specifically, the most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'blue': 3, 'red': 2, 'green': 1})\n",
      "[('blue', 3), ('red', 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Example\n",
    "count = Counter()\n",
    "for word in ['red', 'blue', 'red', 'green', 'blue', 'blue']:\n",
    "    count[word] += 1\n",
    "print(count)\n",
    "print(count.most_common(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 32880),\n",
       " ('to', 28810),\n",
       " ('the', 28088),\n",
       " ('a', 21321),\n",
       " ('you', 21180),\n",
       " ('i', 16000),\n",
       " ('and', 14565),\n",
       " ('it', 12819),\n",
       " ('my', 12385),\n",
       " ('for', 12149),\n",
       " ('in', 11199),\n",
       " ('is', 11185),\n",
       " ('of', 10326),\n",
       " ('that', 9181),\n",
       " ('on', 9020),\n",
       " ('have', 8991),\n",
       " ('me', 8255),\n",
       " ('so', 7612),\n",
       " ('but', 7220),\n",
       " ('be', 7093)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "# Let's apply the example above to count words in our SentimentText\n",
    "# Your code here\n",
    "for row in sentiment.SentimentText:\n",
    "    for word in row.split():\n",
    "        vocab[word] +=1\n",
    "vocab.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the most common words are meaningless in terms of sentiment: _I, to, the, and_... they don't give any information on positiveness or negativeness. They're basically **noise** that can most probably be eliminated. These kind of words are called _stop words_, and it is a common practice to remove them when doing text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Schaefer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#open lib stopwords in english : I,you,they,it,.....\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 32880),\n",
       " (\"I'm\", 6416),\n",
       " ('like', 5086),\n",
       " ('-', 4922),\n",
       " ('get', 4864),\n",
       " ('u', 4194),\n",
       " ('good', 3953),\n",
       " ('love', 3494),\n",
       " ('know', 3472),\n",
       " ('go', 2990),\n",
       " ('see', 2868),\n",
       " ('one', 2787),\n",
       " ('got', 2775),\n",
       " ('think', 2613),\n",
       " ('&amp;', 2556),\n",
       " ('lol', 2419),\n",
       " ('going', 2396),\n",
       " ('really', 2287),\n",
       " ('im', 2200),\n",
       " ('day', 2195)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "vocab_reduced = Counter()\n",
    "# Go through all of the items of vocab using vocab.items() and pick only words that are not in 'stop' \n",
    "# and save them in vocab_reduced\n",
    "# Your code here\n",
    "for row in sentiment.SentimentText:\n",
    "    for word in row.split():\n",
    "        if word in stop:\n",
    "            continue\n",
    "        vocab_reduced[word] +=1\n",
    "vocab_reduced.most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better, only in the 20 most common words we already see words that make sense: good, love, really... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing special characters and \"trash\"\n",
    "\n",
    "If you look closer, you'll see that we're also taking into consideration punctuation signs ('-', ',', etc) and other html tags like `&amp`. We can definitely remove them for the sentiment analysis, but we will try to keep the emoticons, since those _do_ have a sentiment load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test d :D\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\" Return a cleaned version of text\n",
    "    \"\"\"\n",
    "    # Remove HTML markup\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # removing the nose character for standarization. Convert to lower case\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Create some random texts for testing the function preprocessor()\n",
    "print(preprocessor('@test :D'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready! There is another trick we can use to reduce our vocabulary and consolidate words. If you think about it, words like: love, loving, etc. _Could_ express the same positivity. If that was the case, we would be  having two words in our vocabulary when we could have only one: lov. This process of reducing a word to its root is called **stemming**.\n",
    "\n",
    "We also need a _tokenizer_ to break down our twits in individual words. We will implement two tokenizers, a regular one and one that does steaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'there,', 'I', 'am', 'loving', 'this,', 'like', 'with', 'a', 'lot', 'of', 'love']\n",
      "['Hi', 'there,', 'I', 'am', 'love', 'this,', 'like', 'with', 'a', 'lot', 'of', 'love']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer() #loved loveing love =>love\n",
    "\n",
    "# write a function called `tokenizer()` that split a text into list of words\n",
    "def tokenizer(text):\n",
    "    token = [] \n",
    "    # Your code here\n",
    "    token = text.split()\n",
    "    return token\n",
    "\n",
    "\n",
    "# write a function named `tokenizer_porter()` that split a text into list of words and apply stemming technic\n",
    "# Hint: porter.stem(word)\n",
    "def tokenizer_porter(text):\n",
    "    token = [porter.stem(word) for word in text.split()]\n",
    "    # Your code here\n",
    "    \n",
    "    return token\n",
    "\n",
    "# Testing\n",
    "#WHY STILL HAVE THIS, HERE .... AND THIS, is difference with THIS. ???????\n",
    "print(tokenizer('Hi there, I am loving this, like with a lot of love'))\n",
    "print(tokenizer_porter('Hi there, I am loving this, like with a lot of love'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in train and test\n",
    "# Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = sentiment[\"SentimentText\"]# 1 dimension h\n",
    "y = sentiment[\"Sentiment\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stop,\n",
    "                        tokenizer=tokenizer_porter,\n",
    "                        preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Classification\n",
    "\n",
    "We are finally ready to train our algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function prepr... penalty='l2', random_state=101, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# A pipeline is what chains several steps together, once the initial exploration is done. \n",
    "# For example, some codes are meant to transform features — normalise numericals, or turn text into vectors, \n",
    "# or fill up missing data, they are transformers; other codes are meant to predict variables by fitting an algorithm,\n",
    "# they are estimators. Pipeline chains all these together which can then be applied to training data\n",
    "clf = Pipeline([('vect', tfidf),\n",
    "                ('clf', LogisticRegression(random_state=101))])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Now apply those above metrics to evaluate your model\n",
    "# Your code here\n",
    "prediction = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run some tests :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is really bad --> Negative, Positive = [0.96 0.04]\n",
      "I love this! --> Negative, Positive = [0.08 0.92]\n",
      ":) --> Negative, Positive = [0.38 0.62]\n"
     ]
    }
   ],
   "source": [
    "twits = [\n",
    "    \"This is really bad\",\n",
    "    \"I love this!\",\n",
    "    \":)\",\n",
    "]\n",
    "\n",
    "preds = clf.predict_proba(twits)\n",
    "\n",
    "for i in range(len(twits)):\n",
    "    print(f'{twits[i]} --> Negative, Positive = {preds[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we would like to use the classifier in another place, or just not train it again and again everytime, we can save the model in a pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "pickle.dump(clf, open(os.path.join('data', 'logisticRegression.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And you're done! I hope you liked this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bonus: Confusion Matrices\n",
    "\n",
    "https://towardsdatascience.com/taking-the-confusion-out-of-confusion-matrices-c1ce054b3d3e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy, pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use 2 models to predict if a woman is pregnant or not and save the result in \"pregnant.csv\" files.\n",
    "* pregnent dataset has 3 columns:\n",
    "    * Actual is actual result (ground truth)\n",
    "    * model_1_predict, model_2_predict are prediction results from model 1 and model 2\n",
    "\n",
    "**Now, we have to answer the question which model is better?**\n",
    "\n",
    "First of all, let's load dataset and print it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>model_1_predict</th>\n",
       "      <th>model_2_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Actual  model_1_predict  model_2_predict\n",
       "0       0                0                0\n",
       "1       1                0                1\n",
       "2       0                0                0\n",
       "3       0                0                0\n",
       "4       0                0                0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pregnant = # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate True Positive, False Positive, True Negative, False Negative of model 1\n",
    "![Alt](https://cdn-images-1.medium.com/max/800/1*g5zpskPaxO8uSl0OWT4NTQ.png)\n",
    "Note\n",
    "* Positive : 1(Pregnant)\n",
    "* Negative : 0(Not Pregnant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Positive : Predict **Positive** and it's **True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict Positive\n",
    "positive = # Your code here\n",
    "# It's True (correct prediction)\n",
    "correct_predict = # Your code here\n",
    "\n",
    "TP = sum(positive & correct_predict)\n",
    "TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Positive: Predict **Positive** and it's **False**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict Positive\n",
    "positive = # Your code here\n",
    "# It's False (incorrect prediction)\n",
    "incorrect_predict = # Your code here\n",
    "\n",
    "FP = sum(positive & incorrect_predict)\n",
    "FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Negative : Predict **Negative** and it's **True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict Negative\n",
    "negative = # Your code here\n",
    "# It's True (correct prediction)\n",
    "correct_predict = # Your code here\n",
    "\n",
    "TN = sum(negative & correct_predict)\n",
    "TN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Negative: Predict **Negative** and it's **False**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict Negative\n",
    "negative = # Your code here\n",
    "# It's False (incorrect prediction)\n",
    "incorrect_predict = # Your code here\n",
    "\n",
    "FN = sum(negative & incorrect_predict)\n",
    "FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy vs Precision vs Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt img](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/440px-Precisionrecall.svg.png)\n",
    "\n",
    "- Accuracy (all correct / all) = (TP + TN) / (TP + TN + FP + FN)\n",
    "- Precision (true positives / predicted positives) = TP / (TP + FP)\n",
    "- Sensitivity aka Recall (true positives / all actual positives) = TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.85\n",
      "Precision:  0.38095238095238093\n",
      "RecalL   :  0.8\n"
     ]
    }
   ],
   "source": [
    "Accuracy = # Your code here\n",
    "Precision = # Your code here\n",
    "Recall = # Your code here\n",
    "print(\"Accuracy : \", Accuracy)\n",
    "print(\"Precision: \", Precision)\n",
    "print(\"RecalL   : \", Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank to Sklearn for making our life more easier with classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(df_pregnant['Actual'], df_pregnant['model_1_predict'])\n",
    "report = classification_report(df_pregnant['Actual'], df_pregnant['model_1_predict'])\n",
    "print(confusion)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[395  55]\n",
      " [  5  45]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.88      0.93       450\n",
      "          1       0.45      0.90      0.60        50\n",
      "\n",
      "avg / total       0.93      0.88      0.90       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply the same with model 2\n",
    "confusion = confusion_matrix(# Your code here)\n",
    "report = classification_report(# Your code here)\n",
    "print(confusion)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
